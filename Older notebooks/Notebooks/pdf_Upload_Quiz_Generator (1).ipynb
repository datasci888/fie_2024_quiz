{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWICtvbsRLAn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install pypdf langchain_community rapidocr-onnxruntime langchain_openai faiss-cpu langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLqhOkItRLAq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3ld-CJoRLAr"
      },
      "source": [
        "#### Assign the Open AI Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4mS14cVRLAt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwe-l6BJRLAu"
      },
      "source": [
        "#### Give the path of the PDF Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ0afDU2RLAu"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"document.pdf\", extract_images=True)\n",
        "pages = loader.load()\n",
        "# pages[4].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7jov5GRLAv"
      },
      "source": [
        "#### Create Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fidWtT--RLAv"
      },
      "outputs": [],
      "source": [
        "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDL-Kk15RLAw"
      },
      "outputs": [],
      "source": [
        "retriever = faiss_index.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v-fPUyyRLAw"
      },
      "source": [
        "#### Assign the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xUqnEkgRLAw"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZfjGSoRRLAw"
      },
      "source": [
        "#### Create a History aware retriever chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oioOlx3vRLAy"
      },
      "outputs": [],
      "source": [
        "# First we need a prompt that we can pass into an LLM to generate this search query\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
        "])\n",
        "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maJVJ0ItRLAy"
      },
      "source": [
        "#### Create a pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoMaPZs2RLAy"
      },
      "outputs": [],
      "source": [
        "### Answer question ###\n",
        "qa_system_prompt = \"\"\"\n",
        "You are an assistant for creating MCQ questions based on the context provided. \\\n",
        "Use the following pieces of retrieved context to formulate the MCQ question. \\\n",
        "Don't repeat the question. \\\n",
        "Display one MCQ question and options only at a time. \\\n",
        "Question in one line and options in separate line and wait for the user Answer. \\\n",
        "If user answer correct say your answer is correct, and ask would you like to try another question from same topic.\\\n",
        "If user answer is wrong say your answer is wrong and display the correct answer with explanation, and ask would you like to try another question from same topic.\\\n",
        "If you don't know the answer, just say that you don't know. \\\n",
        "Use three sentences maximum and keep the answer concise.\\\n",
        "\n",
        "{context}\"\"\"\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "\n",
        "rag_chain = create_retrieval_chain(retriever_chain, question_answer_chain)\n",
        "\n",
        "\n",
        "### Statefully manage chat history ###\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "\n",
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGqcIZBMRLA0"
      },
      "source": [
        "#### Invoke the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt7alpIrRLA0",
        "outputId": "a2b8fea3-bf8b-4d77-90aa-f25e3fd7c8e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 83278ad2-ae2d-4e8e-9ce7-2aa47150ed70 not found for run e7f87cba-b086-41c8-aa5f-994ddcddf98f. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the focus of the attention mechanism in the encoder self-attention at layer 5 of 6 in the provided context?\n",
            "A) Verb 'making'\n",
            "B) Noun 'laws'\n",
            "C) Adjective 'difficult'\n",
            "D) Adverb 'more'\n",
            "E) Preposition 'of'\n",
            "\n",
            "Answer the question by selecting one of the options (A, B, C, D, E).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 8e0d644e-8e29-47f9-a2cd-5f2fc7629ba1 not found for run 0e4f4253-f223-4059-b38e-1127267f68a2. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your answer is correct!\n",
            "\n",
            "Would you like to try another question from the same topic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 0c587ed8-4fec-41fb-8792-50f71497c3e8 not found for run 770577c7-8441-4941-925d-388c7d6cec41. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the additional sub-layer inserted in each decoder layer compared to the encoder layer in the Transformer model architecture?\n",
            "A) Multi-head self-attention mechanism\n",
            "B) Position-wise fully connected feed-forward network\n",
            "C) Multi-head attention over the output of the encoder stack\n",
            "D) Layer normalization\n",
            "E) Residual connection\n",
            "\n",
            "Answer the question by selecting one of the options (A, B, C, D, E).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 8144aac3-646c-4121-8448-d76ae3400c85 not found for run 7fbf5ca7-4ccb-42db-a83b-ef4b5f82b495. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your answer is incorrect.\n",
            "\n",
            "The correct answer is:\n",
            "C) Multi-head attention over the output of the encoder stack\n",
            "\n",
            "In the decoder of the Transformer model architecture, an additional sub-layer is inserted in each decoder layer compared to the encoder layer, which performs multi-head attention over the output of the encoder stack.\n",
            "\n",
            "Would you like to try another question from the same topic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 6cbbd6f5-628f-45c4-b14a-f37f45725da1 not found for run a222e173-d633-472c-a327-dca6acbc44ea. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the primary focus of the Transformer model architecture described in the provided context?\n",
            "A) Recurrent layers\n",
            "B) Convolutional layers\n",
            "C) Attention mechanism\n",
            "D) Fully connected layers\n",
            "E) Positional encoding\n",
            "\n",
            "Answer the question by selecting one of the options (A, B, C, D, E).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run eaa5d9ea-8b38-488b-9780-d9426ecd06aa not found for run 7e367c29-3102-4459-82c1-ae0e4b67ebca. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your answer is correct!\n",
            "\n",
            "Would you like to try another question from the same topic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run eed83a6a-1475-41bf-89f5-7a59d1737178 not found for run c60ffca0-9a42-4ab6-b68e-135478e9eeda. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the key feature of the Transformer model architecture that sets it apart from traditional sequence transduction models?\n",
            "A) Recurrent layers\n",
            "B) Convolutional layers\n",
            "C) Attention mechanism\n",
            "D) Fully connected layers\n",
            "E) Positional encoding\n",
            "\n",
            "Answer the question by selecting one of the options (A, B, C, D, E).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 783c7e03-8717-4176-b09d-169941164bfa not found for run 76bced93-d301-4a02-be6c-d1f9f5aaed4d. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your answer is correct!\n",
            "\n",
            "Would you like to try another question from the same topic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 620300e4-9c28-4b99-918d-568a25a5bdc4 not found for run 2f96b3dc-53d4-40c1-9d57-74c1d7576425. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the Transformer model architecture, what type of mechanism is used to compute representations of input and output without using sequence-aligned RNNs or convolution?\n",
            "A) Self-attention\n",
            "B) Recurrent attention\n",
            "C) Convolutional attention\n",
            "D) Positional attention\n",
            "E) Layer normalization\n",
            "\n",
            "Answer the question by selecting one of the options (A, B, C, D, E).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run d01df3a5-f529-4103-a8a0-7314fdd754d6 not found for run dc450ead-a81f-4232-b5a6-5cbff0dc3b9b. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your answer is correct!\n",
            "\n",
            "Would you like to try another question from the same topic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 5a2c7111-acfe-4d92-96d4-0ea71903168b not found for run 021370df-a63b-43fb-b86c-ffb169d6b282. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the Transformer model architecture, what is the primary function of the encoder stack?\n",
            "A) Generating output sequence symbols\n",
            "B) Mapping input sequence symbols to continuous representations\n",
            "C) Performing multi-head attention over the output of the encoder stack\n",
            "D) Preventing positions from attending to subsequent positions\n",
            "E) Applying layer normalization around sub-layers\n",
            "\n",
            "Answer the question by selecting one of the options (A, B, C, D, E).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run fa44da8d-139f-4bb1-97dc-07adf087d3d7 not found for run 5274c927-a6eb-4066-a82f-c1df5be78745. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, but your response is incomplete. Please select one of the options (A, B, C, D, E) to answer the question.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 159388df-577e-4de5-bf1c-af331e202ca4 not found for run a2086b5e-6678-41f7-8d5c-e7a14976b198. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your answer is incorrect.\n",
            "\n",
            "The correct answer is:\n",
            "B) Mapping input sequence symbols to continuous representations\n",
            "\n",
            "In the Transformer model architecture, the primary function of the encoder stack is to map an input sequence of symbol representations to a sequence of continuous representations.\n",
            "\n",
            "Would you like to try another question from the same topic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run a38544a2-3066-498a-a4cd-cefd45d20004 not found for run db155c00-b478-41ef-b8bc-6958f0ef2fb4. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the key feature of the Transformer model architecture that allows for significantly more parallelization compared to models using recurrent layers?\n",
            "A) Self-attention mechanism\n",
            "B) Recurrent connections\n",
            "C) Positional encoding\n",
            "D) Layer normalization\n",
            "E) Fully connected layers\n",
            "\n",
            "Answer the question by selecting one of the options (A, B, C, D, E).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 3b55d325-6a96-45be-afbf-a70d4b095537 not found for run d840f82d-e0bd-4a02-a94a-7503c591e950. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your answer is correct!\n",
            "\n",
            "Would you like to try another question from the same topic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 1d920bd7-f4fe-4d5f-84aa-27df11366782 not found for run 877c2f8e-6367-4b5b-94f4-0417291b71b2. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the primary mechanism used in the Transformer model architecture to draw global dependencies between input and output?\n",
            "A) Recurrent layers\n",
            "B) Convolutional layers\n",
            "C) Attention mechanism\n",
            "D) Fully connected layers\n",
            "E) Positional encoding\n",
            "\n",
            "Answer the question by selecting one of the options (A, B, C, D, E).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 34ee14cd-adde-4dee-9ab2-c46c2cba8e9a not found for run fcafef5e-9e9c-42ae-b440-1a3a55845f21. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your answer is correct!\n",
            "\n",
            "Would you like to try another question from the same topic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run cacaecfb-26e8-494c-b5cf-1fbe7aacf126 not found for run 754f3bdf-3cad-4df7-881d-beb650ca7689. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the key advantage of the Transformer model architecture over models using recurrent or convolutional layers for translation tasks?\n",
            "A) Faster training\n",
            "B) Higher memory utilization\n",
            "C) Lower computational complexity\n",
            "D) Improved interpretability\n",
            "E) Enhanced regularization\n",
            "\n",
            "Answer the question by selecting one of the options (A, B, C, D, E).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run 83743fbd-b0bf-4ca2-9a8d-665213c19c50 not found for run 22b71861-eccc-47d5-bb2f-d465ff78649e. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your answer is incorrect.\n",
            "\n",
            "The correct answer is:\n",
            "A) Faster training\n",
            "\n",
            "The key advantage of the Transformer model architecture over models using recurrent or convolutional layers for translation tasks is that it can be trained significantly faster.\n",
            "\n",
            "Would you like to try another question from the same topic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parent run d00e56a9-c955-40b6-afe2-118e1cd2f47a not found for run 8fe1681b-fda3-4da6-858d-6da0aacfafd2. Treating as a root run.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the primary focus of the attention mechanism in the encoder self-attention at layer 5 of 6 in the provided context?\n",
            "A) Verb 'making'\n",
            "B) Noun 'laws'\n",
            "C) Adjective 'difficult'\n",
            "D) Adverb 'more'\n",
            "E) Preposition 'of'\n",
            "\n",
            "Answer the question by selecting one of the options (A, B, C, D, E).\n",
            "bye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    user_input = input(\"Enter the response: \")\n",
        "    if user_input == \"exit\":\n",
        "        print(\"bye!\")\n",
        "        break\n",
        "    print(conversational_rag_chain.invoke( {\"input\": user_input}, config={\"configurable\": {\"session_id\": \"abc123\"}},)[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aMAuOGuRLA1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}