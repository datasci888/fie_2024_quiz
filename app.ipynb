{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de070fa-1628-4ae2-af65-3a3dc33ca74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from langchain.vectorstores import Vectara\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ab4551-2312-4a6b-84ff-b06f63244b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config settings for OpenAI and Vectra\n",
    "customer_id = '189024573'\n",
    "corpus_id = '2'\n",
    "api_key = 'PROVIDE VECTRA API KEY OR ACCESS THROUGH CONFIG FILE'\n",
    "openai_key = 'PROVIDE OPENAI API KEY OR ACCESS THROUGH CONFIG FILE'\n",
    "os.environ[\"VECTARA_CUSTOMER_ID\"] = customer_id\n",
    "os.environ[\"VECTARA_CORPUS_ID\"] = corpus_id\n",
    "os.environ[\"VECTARA_API_KEY\"] = api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1175be61-0dc3-45d2-9828-ef3edf58ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Retriever\n",
    "vectara = Vectara()\n",
    "summary_config = {\n",
    "    \"is_enabled\": True, \"max_results\": 50,\n",
    "    \"response_lang\": \"en\",\n",
    "    \"prompt_name\": \"Generate MCQ\"\n",
    "}\n",
    "retriever = vectara.as_retriever(\n",
    "    search_kwargs={\"k\": 100, \"summary_config\": summary_config}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e4ec18-543b-4468-9555-a64437327eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = \"\"\"\n",
    "Given a chat history and the user topic \\\n",
    "which might reference context in the chat history, formulate a MCQ question. \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "Recreate the question based on the retrieved documents with out ging same question as it is.\n",
    "\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "qa_system_prompt = \"\"\"\n",
    "You are an assistant for creating MCQ questions based on the context for user. \\\n",
    "Don not display exactly same question, shufle the options and recreate the question. \\ \n",
    "Use the following pieces of retrieved context to formulate the MCQ question. \\\n",
    "Don't repeat the question. \\\n",
    "Display one MCQ question and options only at a time. \\\n",
    "Question in one line and options in separate line and wait for the user Answer. \\\n",
    "Apply adaptive ranking by evaluating the retrieved MCQs, re-ranking them based on difficulty, and sequencing the questions according to user responses.\\ \n",
    "If the user answers correctly, respond with 'Your answer is correct' and, ask would you like to try another question from same topic (Yes/exit).\\\n",
    "If the user answers incorrectly, respond with 'Your answer is wrong' and provide the correct answer with a brief explanation and, ask would you like to try another question from same topic (Yes/exit).\\.\\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d2f63f6-2e97-45b8-bfb7-d8ddb741cf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the response:  animals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What type of animals are found in the phylum Chordata and have a backbone?\n",
      "\n",
      "A: invertebrates\n",
      "B: vertebrates\n",
      "C: arthropods\n",
      "D: mollusks\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the response:  B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your answer is correct! Would you like to try another question from the same topic? (Yes/exit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the response:  Yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the largest mammal on earth?\n",
      "\n",
      "A: giraffe\n",
      "B: tiger shark\n",
      "C: water buffalo\n",
      "D: blue whale\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the response:  A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your answer is incorrect. The correct answer is D: blue whale. Would you like to try another question from the same topic? (Yes/exit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the response:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bye!\n"
     ]
    }
   ],
   "source": [
    "## Calling the model iteratively to give user inputs for model\n",
    "while True:\n",
    "    user_input = input(\"Enter the response: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"bye!\")\n",
    "        break\n",
    "    print(conversational_rag_chain.invoke( {\"input\": user_input}, config={\"configurable\": {\"session_id\": \"abc123\"}},)[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb5393-dab9-4b5d-a65a-a6b7c7dc91df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
